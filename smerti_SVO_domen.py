# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –±—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–º–µ—Ä—Ç–∏
import requests
import json
from datetime import datetime
import os
import socket
import time
import re
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional
import warnings
import gc
import threading
from queue import Queue
from functools import lru_cache
import sys
import locale

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è –≤–≤–æ–¥–∞/–≤—ã–≤–æ–¥–∞
if sys.stdout.encoding is None:
    sys.stdout = open(sys.stdout.fileno(), 'w', encoding='utf-8')
if sys.stdin.encoding is None:
    sys.stdin = open(sys.stdin.fileno(), 'r', encoding='utf-8')


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –≤–≤–æ–¥–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏
def safe_input(prompt):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤–≤–æ–¥ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–æ–∫"""
    try:
        return input(prompt)
    except UnicodeDecodeError:
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
        for encoding in ['cp1251', 'cp866', 'latin1', 'utf-8']:
            try:
                # –ß–∏—Ç–∞–µ–º —Å—ã—Ä—ã–µ –±–∞–π—Ç—ã –∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º
                import sys
                raw = sys.stdin.buffer.readline().decode(encoding).strip()
                return raw
            except:
                continue
        return input(prompt)  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞


warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è transformers
try:
    import torch
    from transformers import (
        AutoTokenizer,
        AutoModelForSequenceClassification,
        pipeline,
        AutoModelForSeq2SeqLM
    )
    from sentence_transformers import SentenceTransformer, util
    import numpy as np

    TRANSFORMERS_AVAILABLE = True
    print("‚úÖ Transformers –¥–æ—Å—Ç—É–ø–Ω—ã")

    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"‚öôÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {torch.cuda.get_device_name(0)}")
        torch.cuda.empty_cache()
    else:
        device = torch.device("cpu")
        print(f"‚öôÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

except ImportError as e:
    print(f"‚ö† Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: {e}")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch sentence-transformers")
    TRANSFORMERS_AVAILABLE = False

# –î–û–ú–ï–ù–´ –ò –ó–ê–Ø–í–ö–ò –î–õ–Ø –ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø
EXCLUDED_DOMAINS = [
    "https://admin.techlegal.ru",
    "https://test1.techlegal.ru",
    "https://test2.techlegal.ru",
    "https://test3.techlegal.ru",
    "https://test5.techlegal.ru",
    "https://test6.techlegal.ru",
    "https://test7.techlegal.ru"
]

# –°–ü–ò–°–û–ö –°–õ–û–í-–ò–°–ö–õ–Æ–ß–ï–ù–ò–ô (–Ω–µ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ —Å–º–µ—Ä—Ç–∏ –¥–æ–ª–∂–Ω–∏–∫–∞)
EXCEPTION_WORDS = [
    "–ì—É–º–µ—Ä–æ–≤–∏—á–∞",
    "–ì—É–º–µ—Ä–æ–≤–Ω–∞",
    "–ö—É—Ä—Ç—É–º–µ—Ä–æ–≤—É",
    "–ì—É–º–µ—Ä–æ–≤–Ω—ã",
    "–ö—É–º–µ—Ä—Ç–∞—É—Å–∫–∞—è",
    "–ö—É–º–µ—Ä—Ç–∞—É",
    "–ì–£–ú–ï–†–û–í",
    "–ì–£–ú–ï–†–û–í–ò–ß",
    "–ö–£–ú–ï–†–¢–ê–£–°–ö–ò–ô",
    "–ö—É–º–µ—Ä—Ç–∞—É—Å–∫–∏–π",
    "–ù—É–º–µ—Ä–∞—Ü–∏—è",
    "–ù–µ—É–º–µ—Ä–∂–∏—Ç—Å–∫–æ–º—É",
    "—à—É–º–µ—Ä–ª–∏–Ω—Å–∫–æ–º—É",
    "–®–£–ú–ï–†–õ–ò–ù–°–ö–ò–ô",
    "–®—É–º–µ—Ä–ª–∏–Ω—Å–∫–æ–≥–æ",
    "–ë–£–ú–ï–†–ê–ù–ì",
    "–ì—É–º–µ—Ä",
    "–ì–£–ú–ï–†",
    "–°–µ–π—Ç—É–º–µ—Ä–æ–≤—É",
    "–ö—É–º–µ—Ä–∫–∏–Ω–∞",
    "–°—É–º–µ—Ä—É",
    "–ù–µ—É–º–µ—Ä–∂–∏—Ç—Å–∫–∏–π",
    "–ë—É–º–µ—Ä–∞–Ω–≥",
    "–ö—É—Ä—Ç—É–º–µ—Ä–æ–≤–∞",
    "–°–µ–π—Ç—É–º–µ—Ä–æ–≤",
    "–®—É–º–µ—Ä–ª–∏–Ω—Å–∫–æ–µ",
    "–°—É–º–µ—Ä–∞",
    "–Ω—É–º–µ—Ä–∞—Ü–∏–∏",
    "–ö–£–º–µ—Ä—Ç–∞—É—Å–∫–∏–π",
    "–ì—É–º–µ—Ä–∞",
    "–ù–µ—É–º–µ—Ä–∂–∏—Ç—Å–∫–æ–≥–æ",
    "–ì–£–ú–ï–†–û–í–£",
    "–ö—É—Ä—Ç—É–º–µ—Ä–æ–≤",
    "–®—É–º–µ—Ä–∏–Ω–æ–π",
    "–∫—É–º–µ—Ä—Ç–∞—É",
    "–ö–£–ú–ï–†–¢–ê–£",
    "–®–£–ú–ï–†–õ–Ø",
    "–ì–£–ú–ï–†–ê",
    "–ì–£–ú–ï–†–û–í–ò–ß–ê",
    "–®—É–º–µ—Ä",
    "–ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω—ã",
    "–ù–£–ú–ï–†–ê–¶–ò–Ø",
    "–ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω",
    "–°—É–º–µ—Ä–∏–Ω–∞",
    "–∫—É–º–µ—Ä—Ç–∞—É—Å–∫–∏–π",
    "–ì—É–º–µ—Ä–æ–≤–∏—á—É",
    "–¥–æ–∫—É–º–µ—Ä–Ω—Ç–∞",
    "–¥–æ–∫—É–º–µ—Ä—Ç–∞",
    "–ö—É–º–µ—Ä—Ç–∞—É—Å–∫–∏–º",
    "–°–µ–π—Ç—É–º–µ—Ä–æ–≤–∞",
    "–ì—É–º–µ—Ä–æ–≤—É",
    "–®—É–º–µ—Ä–ª–∏",
    "–°–µ–π—Ç—É–º–µ—Ä–∞",
    "–°—É–º–µ—Ä",
    "–ì—É–º–µ—Ä–æ–≤–æ–π",
    "–ì–£–ú–ï–†–û–í–ê",
    "–Ω—É–º–µ—Ä–∞—Ü–∏—è",
    "–¢–£–ú–ï–†–°–û–õ–ê",
    "–°—É–º–µ—Ä–∏–Ω–æ–π",
    "–®—É–º–µ—Ä–ª–∏–Ω—Å–∫–æ–º—É",
    "–ì—É–º–µ—Ä–æ–≤–æ",
    "–£–º–µ—Ä–±–µ–∫–æ–≤–∏—á",
    "–ì—É–º–µ—Ä–æ–≤–∞",
    "–ì—É–º–µ—Ä–æ–≤",
    "–ì–£–ú–ï–†–û–í–û–ô",
    "–®—É–º–µ—Ä–ª–∏–Ω—Å–∫–∏–º",
    "–®—É–º–µ—Ä–ª—è",
    "–ì—É–º–µ—Ä–æ–≤–Ω–µ",
    "–ì–£–ú–ï–†–û–í–ù–ê",
    "–ì—É–º–µ—Ä–æ–≤–∏—á",
    "–¢—É–º–µ—Ä–∫–∏–Ω–æ–π",
    "–¢—É–º–µ—Ä—Å–æ–ª–∞",
    "–ß—É–º–µ—Ä—Ç–æ–≤",
    "–ê–¥–∂–∏—É–º–µ—Ä–æ–≤",
    "–ú–ï–ù–£–ú–ï–†–û–í",
    "–°—É–º–µ—Ä–∏–Ω",
    "–£–º–µ—Ä–µ–Ω–∫–æ–≤–æ–π",
    "–£–º–µ—Ä–µ–Ω–∫–æ–≤–∞",
    "–£–º–µ—Ä–µ–Ω–∫–æ–≤",
    "–£–º–µ—Ä–µ–Ω–∫–æ",
    "–£–º–µ—Ä–µ–Ω–∫–æ–≤—É",
    "–£–º–µ—Ä–æ–≤",
    "–£–º–µ—Ä–æ–≤—É",
    "–£–º–µ—Ä–æ–≤–∞",
    "–£–º–µ—Ä–æ–≤–æ–π",
    "–£–ú–ï–†–û–í–û–ô",
    "–£–ú–ï–†–û–í–ê",
    "–£–ú–ï–†–ï–ù–ö–û",
    "–£–ú–ï–†–û–í",
    "–£–º–µ—Ä–±–∞–µ–≤–∞",
    "–£–º–µ—Ä–∑–∞–∫–æ–≤–∞",
    "–£–º–µ—Ä–Ω–∏–∫–æ–≤–∞",
    "–£–º–µ—Ä–Ω–∏–∫–æ–≤–æ–π",
    "–£–º–µ—Ä–±–µ–∫–æ–≤–∏—á",
    "–£–º–µ—Ä–æ–≤–∏—á",
    "–£–º–µ—Ä–æ–≤–∏—á–∞",
    "–£–ú–ï–†–ë–ê–ï–í–ê",
    "–£–ú–ï–†–ë–ï–ö–û–í–ò–ß",
    "–°—É–º–µ—Ä–∫–∏–Ω–∞"
]

# –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –≥—Ä—É–ø–ø
GROUP_VALUES = [
    "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å",
    "–û—Ç–∫–∞–∑, –∏—Å–ø–æ–ª–Ω–µ–Ω—ã —Ä–∞–Ω–µ–µ",
    "–û—Ç–∫–∞–∑, —Å–º–µ—Ä—Ç—å—é –¥–æ–ª–∂–Ω–∏–∫–∞ –≤—ã–Ω–µ—Å–µ–Ω–æ —Ä–∞–Ω–µ–µ",
    "–û—Ç–∫–∞–∑, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã",
    "–û—Ç–∫–∞–∑, –¥–æ–ª–∂–Ω–∏–∫ —É–≤–æ–ª–µ–Ω",
    "–û—Ç–∫–∞–∑, –¥–æ–ª–∂–Ω–∏–∫ —É–º–µ—Ä",
    "–û—Ç–∫–∞–∑, —Å–º–µ—Ä—Ç–∏ –¥–æ–ª–∂–Ω–∏–∫–∞",
    "–û—Ç–∫–∞–∑, –¥–æ–ª–∂–Ω–∏–∫ —Å—á–∏—Ç–∞–µ—Ç—Å—è —É–º–µ—Ä—à–∏–º",
    "–û—Ç–∫–∞–∑, —Å–º–µ—Ä—Ç—å—é –¥–æ–ª–∂–Ω–∏–∫–∞",
    "–û—Ç–∫–∞–∑, –∏–ø –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ",
    "–û—Ç–∫–∞–∑, –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ",
    "–û—Ç–∫–∞–∑, –æ—Ç–±—ã–≤–∞–µ—Ç –Ω–∞–∫–∞–∑–∞–Ω–∏–µ",
    "–û—Ç–∫–∞–∑, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã",
    "–û—Ç–∫–∞–∑, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ –Ω–µ —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–µ–Ω",
    "–û—Ç–∫–∞–∑, –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–æ—Ö–æ–¥–∞",
    "–û—Ç–∫–∞–∑, –Ω–µ–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã",
    "–û—Ç–∫–∞–∑, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏ –ø–æ–ª–Ω–æ–º–æ—á–∏–π",
    "–û—Ç–∫–∞–∑, –∞—Ä–µ—Å—Ç –Ω–µ –≤–æ–∑–º–æ–∂–µ–Ω",
    "–û—Ç–∫–∞–∑, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ –ø–æ–ª–Ω–æ–º–æ—á–∏—è",
    "–û—Ç–∫–∞–∑, —É–º–µ—Ä",
    "–û—Ç–∫–∞–∑, –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤–ª–æ–∂–µ–Ω–∏–µ",
    "–û—Ç–∫–∞–∑, –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞—è–≤–ª–µ–Ω–∏–µ –∏ –≤–ª–æ–∂–µ–Ω–∏–µ",
    "–û—Ç–∫–∞–∑, –Ω–µ —É–≤–µ–¥–æ–º–ª–µ–Ω",
    "–û—Ç–∫–∞–∑, –æ—Ç–º–µ–Ω–∞ —Å—É–¥–µ–±–Ω–æ–≥–æ –ø—Ä–∏–∫–∞–∑–∞",
    "–û—Ç–∫–∞–∑, –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–æ",
    "–û—Ç–∫–∞–∑, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏",
    "–û—Ç–∫–∞–∑, –Ω–µ –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω",
    "–û—Ç–∫–∞–∑, –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –æ–∫–æ–Ω—á–µ–Ω–æ",
    "–û—Ç–∫–∞–∑, –≤–ª–æ–∂–µ–Ω–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞—è–≤–ª–µ–Ω–∏–µ",
    "–û—Ç–∫–∞–∑, –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ö–æ–¥–∞—Ç–∞–π—Å—Ç–≤–æ",
    "–û—Ç–∫–∞–∑, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å–≤–µ–¥–µ–Ω–∏—è –ø–æ–¥–ª–µ–∂–∞—â–∏–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—é",
    "–û—Ç–∫–∞–∑, –Ω–µ—Å–æ—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç–∏",
    "–û—Ç–∫–∞–∑, –∏–º–µ–µ—Ç—Å—è –¥–µ–π—Å—Ç–≤—É—é—â–µ–µ –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ",
    "–û—Ç–∫–∞–∑, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–ª—É—á–∞—Ç–µ–ª–µ–º –ø–µ–Ω—Å–∏–∏",
    "–û—Ç–∫–∞–∑, –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–∏ –¥–æ–ª–∂–Ω–∏–∫–∞ –ø—Ä–∞–≤–∞ –≤—ã–µ–∑–¥–∞",
    "–û—Ç–∫–∞–∑, –≤–ª–æ–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å –æ—à–∏–±–∫–æ–π",
    "–û—Ç–∫–∞–∑, –Ω–∞—á–∏—Å–ª–µ–Ω—ã —Å—Ç—Ä–∞—Ö–æ–≤—ã–µ",
    "–û—Ç–∫–∞–∑, –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—ã–Ω–µ—Å–µ–Ω–æ",
    "–û—Ç–∫–∞–∑, –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç –æ—Å–Ω–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è",
    "–û—Ç–∫–∞–∑, –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–æ",
    "–û—Ç–∫–∞–∑, –Ω–µ –∏—Å–ø–æ–ª–Ω–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Å",
    "–û—Ç–∫–∞–∑, –≤ —Å–≤—è–∑–∏ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–æ–π",
    "–û—Ç–∫–∞–∑, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–∏–∫–∞",
    "–û—Ç–∫–∞–∑, –±—ã–ª–æ –æ–∫–æ–Ω—á–µ–Ω–æ",
    "–û—Ç–∫–∞–∑, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º –º–∞—Ä–æ—á–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏",
    "–û—Ç–∫–∞–∑, –æ–±—ä—è–≤–ª–µ–Ω–∏–∏ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–æ–∑—ã—Å–∫–∞",
    "–û—Ç–∫–∞–∑, —Å–º–µ—Ä—Ç—å—é —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–º–æ—á–∏–π –∑–∞—è–≤–∏—Ç–µ–ª—è",
    "–û—Ç–∫–∞–∑, –∏–º—É—â–µ—Å—Ç–≤–æ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ",
    "–û—Ç–∫–∞–∑, –æ–±—ä–µ–¥–µ–Ω–∏–Ω—ã –≤ —Å–≤–æ–¥–Ω–æ–µ –ø–æ –¥–æ–ª–∂–Ω–∏–∫—É",
    # "–î–æ–ª–∂–Ω–∏–∫ –°–í–û",
    "—Å–º–µ—Ä—Ç—å—é –¥–æ–ª–∂–Ω–∏–∫–∞",
    "–æ–±—Ä–∞—â–µ–Ω–∏–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–æ",
    "–∑–∞–ø—Ä–æ—Å—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –æ—Ä–≥–∞–Ω—ã",
    "–ø–æ–≤—Ç–æ—Ä–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã",
    "–ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç—ã –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ",
    "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–∏–Ω—è—Ç–∞ –∫ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—é",
    "–Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –ø—Ä–∏—á–∏–Ω–∞–º",
    "–ø–æ–≤—Ç–æ—Ä–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –∑–∞–ø—Ä–æ—Å—ã",
    "–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –≤–∞–º —Å–≤–æ–¥–∫—É –ø–æ –∏–ø",
    "–ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è –≤–µ—Å—å –∫–æ–º–ø–ª–µ–∫—Å –º–µ—Ä",
    "–∏–º—É—â–µ—Å—Ç–≤–æ –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ",
    "–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –æ–∫–æ–Ω—á–µ–Ω–æ",
    "–ø—Ä–∏–æ–±—â–µ–Ω–æ –∫ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º",
    "–ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω –∫ –Ω–∞—Å—Ç–æ—è—â–µ–º—É —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—é",
    "–¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π",
    "–∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
    "–æ–∫–æ–Ω—á–∞–Ω–∏–∏ –∏–ø",
    "–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–º—É—â–µ—Å—Ç–≤–æ",
    "—Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–º–æ—á–∏–π –∑–∞—è–≤–∏—Ç–µ–ª—è",
    "–ø—Ä–∏–Ω—è—Ç—ã –º–µ—Ä—ã –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è",
    "–Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –≤ —Å–≤—è–∑–∏ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º –º–∞—Ä–æ—á–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏",
    "–∏–¥ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
    "–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ —Å–ø—Ä–∞–≤–∫–∞",
    "–¥–æ–ª–∂–Ω–∏–∫ –Ω–µ —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–µ–Ω",
    "–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –≤ –≤–∞—à –∞–¥—Ä–µ—Å —Å–≤–µ–¥–µ–Ω–∏—è –æ —Ö–æ–¥–µ –ò–ü",
    "–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ä–æ–∑—ã—Å–∫–µ",
    "–ø—Ä–∏–Ω—è—Ç–æ –∫ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—é",
    "—Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ –ø–æ–¥–ø–∏—Å–∞–Ω–Ω–æ–µ –≤–ª–æ–∂–µ–Ω–∏–µ",
    "–æ–±–Ω–æ–≤–ª–µ–Ω—ã –∑–∞–ø—Ä–æ—Å—ã",
    "–æ–±–Ω–æ–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞",
    "–∏–ø –æ–∫–æ–Ω—á–µ–Ω–æ",
    "—Ñ–∞–π–ª –Ω–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è",
    "–ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –∞–¥—Ä–µ—Å—É –Ω–µ –ø—Ä–æ–∂–∏–≤–∞–µ—Ç",
    "—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É—é—â–∏–µ –æ—Ä–≥–∞–Ω—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã",
    "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–∏–æ–±—â–µ–Ω–∞ –∫ –∏–ø",
    "–≤ –æ—Ç–ø—É—Å–∫–µ –ø–æ —É—Ö–æ–¥—É –∑–∞ —Ä–µ–±–µ–Ω–∫–æ–º",
    "—ç–ø –Ω–µ –≤–µ—Ä–Ω–∞",
    "–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –¥–µ–π—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∫–≤–µ–∑–∏—Ç—ã",
    "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∑–∞—è–≤–∏—Ç–µ–ª—è –∏—Å–ø–æ–ª–Ω–µ–Ω—ã",
    "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—ã–Ω–µ—Å–µ–Ω–æ",
    "–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —É–¥–µ—Ä–∂–∞–Ω–∏–µ –≤ –ø–µ–Ω—Å–∏–æ–Ω–Ω—ã–π —Ñ–æ–Ω–¥",
    "–∑–∞–ø—Ä–æ—Å—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã",
    "–≤—Å–µ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ –≤ —Å–≤–æ–¥–Ω–æ–º",
    "—Å–≤–æ–¥–∫–∞ –∏ –¥–≤–∏–∂–µ–Ω–∏–µ –¥—Å –≤–æ –≤–ª–æ–∂–µ–Ω–∏–∏",
    "–∏–ø –≤ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–∏ –Ω–µ –≤—ã–Ω–æ—Å–∏—Ç—Å—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ",
    "–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∞ –∑–∞–º–µ–Ω–∞ –≤–∑—ã—Å–∫–∞—Ç–µ–ª—è",
    "–æ—Ç–∑—ã–≤–æ–º –æ–±—Ä–∞—â–µ–Ω–∏—è –≤–∑—ã—Å–∫–∞—Ç–µ–ª–µ–º",
    "–∏–¥ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —É—Å–ª–æ–≤–∏—è–º",
    "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å",
    "–Ω–µ –ø–æ–¥–ø–∞–¥–∞—é—â–∏–º–∏ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏—é",
    "–æ—Ç–∫–∞–∑–∞—Ç—å"
]

DEATH_GROUPS = {
    "–û—Ç–∫–∞–∑, –¥–æ–ª–¥–∂–Ω–∏–∫ —É–º–µ—Ä",
    "–û—Ç–∫–∞–∑, —Å–º–µ—Ä—Ç–∏ –¥–æ–ª–∂–Ω–∏–∫–∞",
    "–û—Ç–∫–∞–∑, –¥–æ–ª–∂–Ω–∏–∫ —É–º–µ—Ä",
    "–û—Ç–∫–∞–∑, —Å–º–µ—Ä—Ç—å—é –¥–æ–ª–∂–Ω–∏–∫–∞",
    "–û—Ç–∫–∞–∑, —É–º–µ—Ä",
    "—Å–º–µ—Ä—Ç—å—é –¥–æ–ª–∂–Ω–∏–∫–∞"
}

# –°–ü–ò–°–û–ö –°–¢–û–ü-–§–†–ê–ó - –µ—Å–ª–∏ –µ—Å—Ç—å —ç—Ç–∏ —Ñ—Ä–∞–∑—ã, –∑–Ω–∞—á–∏—Ç –¥–æ–ª–∂–Ω–∏–∫ –ñ–ò–í
STOP_PHRASES = [
    '–¥–æ–ª–∂–Ω–∏–∫ —É–º–µ—Ä—à–∏–º, —Å–º–µ–Ω–∏–≤—à–∏–º —Ñ–∞–º–∏–ª–∏—é –Ω–µ –∑–Ω–∞—á–∏—Ç—Å—è',
    '–æ—Ä–≥–∞–Ω—ã –∑–∞–≥—Å–∞ —Å–≤–µ–¥–µ–Ω–∏—è–º–∏ –æ —Å–º–µ—Ä—Ç–∏, —Å–º–µ–Ω–µ —Ñ–∞–º–∏–ª–∏–∏ (–∏–º–µ–Ω–∏, –æ—Ç—á–µ—Å—Ç–≤–∞) –¥–æ–ª–∂–Ω–∏–∫–∞ –Ω–µ —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é—Ç',
    '—Å–æ—Å—Ç–æ—è–Ω–∏—è –æ —Å–º–µ—Ä—Ç–∏ –¥–æ–ª–∂–Ω–∏–∫–∞, –æ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –±—Ä–∞–∫–∞',
    '—Å–≤–µ–¥–µ–Ω–∏—è–º–∏ –æ —Å–º–µ—Ä—Ç–∏ –Ω–µ —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é—Ç',
    '–∑–∞–ø–∏—Å—å –∞–∫—Ç–∞ –æ —Å–º–µ—Ä—Ç–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç',
    '–Ω–µ—Ç —Å–≤–µ–¥–µ–Ω–∏–π –æ —Å–º–µ—Ä—Ç–∏',
    '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–º–µ—Ä—Ç–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç',
    '—Å–º–µ—Ä—Ç—å –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞',
    '–Ω–µ –∑–Ω–∞—á–∏—Ç—Å—è —É–º–µ—Ä—à–∏–º'
]


def contains_exception_word(text: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–æ–≤–∞-–∏—Å–∫–ª—é—á–µ–Ω–∏—è (—Ñ–∞–º–∏–ª–∏–∏, —Ç–æ–ø–æ–Ω–∏–º—ã –∏ —Ç.–¥.)
    –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ —Å–º–µ—Ä—Ç–∏ –¥–æ–ª–∂–Ω–∏–∫–∞
    """
    if not text:
        return False

    text_lower = text.lower()

    for word in EXCEPTION_WORDS:
        if word.lower() in text_lower:
            print(f"‚ö†Ô∏è –ù–ê–ô–î–ï–ù–û –°–õ–û–í–û-–ò–°–ö–õ–Æ–ß–ï–ù–ò–ï: {word}")
            return True

    return False


class NeuralNetworkDeathDetector:
    """–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–º–µ—Ä—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ"""

    def __init__(self):
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–º–µ—Ä—Ç–∏...")
        self.device = device

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º rubert-tiny2 –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
            model_name = "cointegrated/rubert-tiny2-cedr-emotion-detection"

            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.device)
            self.model.eval()

            # –û—Ç–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            if hasattr(self.model, 'requires_grad_'):
                self.model.requires_grad_(False)

            print(f"‚úÖ –ù–µ–π—Ä–æ—Å–µ—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")

            # –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.cache = {}
            self.cache_lock = threading.Lock()
            self.cache_size = 1000

            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            self.death_keywords = [
                '—É–º–µ—Ä', '—Å–º–µ—Ä—Ç—å', '—Å–∫–æ–Ω—á–∞–ª—Å—è', '–ø–æ–≥–∏–±', '–ª–µ—Ç–∞–ª—å–Ω—ã–π',
                '–º–µ—Ä—Ç–∏', '–º–µ—Ä—Ç–≤', '–Ω–µ —Å—Ç–∞–ª–æ', '–ø–æ—Ö–æ—Ä–æ–Ω—ã', '—Ç—Ä—É–ø',
                '—Å–∫–æ–Ω—á–∞–ª–∞—Å—å', '–ø–æ–≥–∏–±–ª–∞', '—É–º–µ—Ä–ª–∞'
            ]

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {e}")
            raise

    def detect_death(self, text: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–º–µ—Ä—Ç–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True/False
        """
        if not text or len(text) < 10:
            return False

        # –ü–†–û–í–ï–†–ö–ê –ù–ê –°–õ–û–í–ê-–ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø
        if contains_exception_word(text):
            print(f"‚úÖ –¢–ï–ö–°–¢ –°–û–î–ï–†–ñ–ò–¢ –°–õ–û–í–ê-–ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø - –¥–æ–ª–∂–Ω–∏–∫ –ñ–ò–í")
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        text_hash = hashlib.md5(text[:500].encode()).hexdigest()
        with self.cache_lock:
            if text_hash in self.cache:
                return self.cache[text_hash]

        # –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –°–¢–û–ü-–§–†–ê–ó
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–æ—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã - –∑–Ω–∞—á–∏—Ç –¥–æ–ª–∂–Ω–∏–∫ –ñ–ò–í
        text_lower = text.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã
        for phrase in STOP_PHRASES:
            if phrase in text_lower:
                print(f"‚úÖ –ù–ê–ô–î–ï–ù–ê –°–¢–û–ü-–§–†–ê–ó–ê: {phrase} - –¥–æ–ª–∂–Ω–∏–∫ –ñ–ò–í")
                with self.cache_lock:
                    if len(self.cache) < self.cache_size:
                        self.cache[text_hash] = False
                return False

        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        has_keywords = any(keyword in text_lower for keyword in self.death_keywords)

        if not has_keywords:
            # –ï—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —Å—Ä–∞–∑—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º False
            with self.cache_lock:
                if len(self.cache) < self.cache_size:
                    self.cache[text_hash] = False
            return False

        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è —Ç–æ—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            text_for_analysis = text[:1000]

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                text_for_analysis,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            ).to(self.device)

            # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.softmax(outputs.logits, dim=-1)

            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            # –ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —ç–º–æ—Ü–∏–π: 0 - –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ, 1 - –≥–Ω–µ–≤, 2 - —Å—Ç—Ä–∞—Ö, –∏ —Ç.–¥.
            # –ù–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —ç–º–æ—Ü–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ —Ç–µ, —á—Ç–æ —Å–≤—è–∑–∞–Ω—ã —Å–æ —Å–º–µ—Ä—Ç—å—é
            probs = predictions[0].cpu().numpy()

            # –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–º–µ—Ä—Ç–∏ (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è)
            DEATH_THRESHOLD = 0.3

            # –ï—Å–ª–∏ –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —ç–º–æ—Ü–∏–π –∏ –µ—Å—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            max_neg_prob = max(probs[1:])  # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—É—é

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–º–µ—Ä—Ç–∏
            death_patterns = [
                r'–≤\s+—Å–≤—è–∑–∏\s+—Å–æ\s+—Å–º–µ—Ä—Ç—å—é',
                r'–ø–æ\s+–ø—Ä–∏—á–∏–Ω–µ\s+—Å–º–µ—Ä—Ç–∏',
                r'—Å–º–µ—Ä—Ç—å\s+–¥–æ–ª–∂–Ω–∏–∫–∞',
                r'–¥–æ–ª–∂–Ω–∏–∫\s+—É–º–µ—Ä',
                r'–≤ —Å–≤—è–∑–∏ —Å–æ —Å–º–µ—Ä—Ç—å—é',
                r'—Å–º–µ—Ä—Ç—å—é –¥–æ–ª–∂–Ω–∏–∫–∞'
            ]

            has_death_pattern = any(re.search(pattern, text_lower) for pattern in death_patterns)

            # –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ
            result = (max_neg_prob > DEATH_THRESHOLD and has_keywords) or has_death_pattern

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            with self.cache_lock:
                if len(self.cache) < self.cache_size:
                    self.cache[text_hash] = result

            return result

        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {e}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
            return has_keywords

    def detect_batch(self, texts: List[str], batch_size: int = 32) -> List[bool]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è"""
        results = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_results = []

            for text in batch:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω–æ—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –±–∞—Ç—á–µ–π
                batch_results.append(self.detect_death(text))

            results.extend(batch_results)

        return results


class FinalTransformersAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å transformers - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""

    def __init__(self):
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ NLP –º–æ–¥–µ–ª–µ–π...")

        try:
            # 1. –ú–æ–¥–µ–ª—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ - –∑–∞–≥—Ä—É–∂–∞–µ–º –æ–¥–∏–Ω —Ä–∞–∑
            print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤...")
            self.summarization_tokenizer = AutoTokenizer.from_pretrained(
                "IlyaGusev/rut5_base_sum_gazeta",
                use_fast=True
            )

            # –£–ø—Ä–æ—â–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è CPU
            self.summarization_model = AutoModelForSeq2SeqLM.from_pretrained(
                "IlyaGusev/rut5_base_sum_gazeta"
            ).to(device)

            # –û—Ç–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            self.summarization_model.eval()
            if hasattr(self.summarization_model, 'requires_grad_'):
                self.summarization_model.requires_grad_(False)

            print("‚úÖ –ú–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

            # 2. –ú–æ–¥–µ–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            self.embedding_model = SentenceTransformer(
                'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                device=str(device)
            )
            self.embedding_model.eval()
            print("‚úÖ –ú–æ–¥–µ–ª—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

            # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–º–µ—Ä—Ç–∏
            try:
                self.death_neural_network = NeuralNetworkDeathDetector()
                print("‚úÖ –ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–º–µ—Ä—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                print(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å–º–µ—Ä—Ç–∏: {e}")
                self.death_neural_network = None

            # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≥—Ä—É–ø–ø
            self._init_group_indexes()

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(GROUP_VALUES)} –≥—Ä—É–ø–ø –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")

            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫—ç—à
            self.cache = {}
            self.cache_max_size = 10000
            self.cache_lock = threading.Lock()

            # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
            self.death_pattern = re.compile(r'—É–º–µ—Ä|—Å–º–µ—Ä—Ç—å|—Å–∫–æ–Ω—á–∞–ª—Å—è|–ø–æ–≥–∏–±|–ª–µ—Ç–∞–ª—å–Ω—ã–π –∏—Å—Ö–æ–¥', re.IGNORECASE)
            self.svo_pattern = re.compile(r'\b–°–í–û\b|\b—Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è\s+–≤–æ–µ–Ω–Ω–∞—è\s+–æ–ø–µ—Ä–∞—Ü–∏—è\b|\b—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è\b',
                                          re.IGNORECASE)

            # –°—Ç—Ä–æ–≥–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –°–í–û - —Ç–æ–ª—å–∫–æ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            self.svo_pattern = re.compile(
                r'(?<![–∞-—è—ë])—Å–≤–æ(?![–∞-—è—ë])|'  # —Å—Ç—Ä–æ–≥–æ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ "—Å–≤–æ" (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
                r'\b–°–í–û\b|'  # –ª–∞—Ç–∏–Ω–∏—Ü–∞/–∑–∞–≥–ª–∞–≤–Ω—ã–µ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ
                r'—Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è\s+–≤–æ–µ–Ω–Ω–∞—è\s+–æ–ø–µ—Ä–∞—Ü–∏—è|'
                r'—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è',
                re.IGNORECASE
            )
            self.number_pattern = re.compile(r'‚Ññ\s*\d+|–ò–ü[-/]\d+')
            self.clean_pattern = re.compile(r'\s+')

            # –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤—ã–∑–≤–∞—Ç—å –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
            self.false_positive_words = [
                '—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ', '—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω', '—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã',  # —Å–æ–¥–µ—Ä–∂–∏—Ç "–Ω–æ–≤"
                '—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–æ',  # —Å–æ–¥–µ—Ä–∂–∏—Ç "—Å–º–æ—Ç—Ä" –∏ "–Ω–æ"
                '–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ', '–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è', '–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏',  # —Å–æ–¥–µ—Ä–∂–∏—Ç "—Å—Ç–∞–Ω–æ–≤"
                '—Å–≤–æ–¥–Ω—ã–π', '—Å–≤–æ–¥–Ω–æ–µ', '—Å–≤–æ–¥–Ω—ã–µ', '—Å–≤–æ–¥–∫–∞',  # —Å–æ–¥–µ—Ä–∂–∏—Ç "—Å–≤–æ"
                '—Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–æ', '—Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ',  # –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "—Å–≤–æ"
                '—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ', '—Å–≤–∏–¥–µ—Ç–µ–ª—å',  # —Å–æ–¥–µ—Ä–∂–∏—Ç "—Å–≤–∏" –∏ "–≤–æ"
                '—É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ', '—É–≤–µ–¥–æ–º–ª–µ–Ω–∏–∏',  # —Å–æ–¥–µ—Ä–∂–∏—Ç "–≤–µ–¥" –∏ "–ª–µ"
            ]

            print(f"‚úÖ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)")

            # –û—á–∏—â–∞–µ–º –∫—ç—à –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            self._determine_group_cached.cache_clear()

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            raise

    def _is_svo_detected(self, text: str) -> bool:
        """
        –£–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –°–í–û –≤ —Ç–µ–∫—Å—Ç–µ
        """
        if not text:
            return False

        text_lower = text.lower()

        # 1. –¢–æ—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã (—Å–∞–º–æ–µ –Ω–∞–¥–µ–∂–Ω–æ–µ)
        exact_phrases = [
            '—É—á–∞—Å—Ç–Ω–∏–∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –≤–æ–µ–Ω–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏',
            '–º–æ–±–∏–ª–∏–∑–æ–≤–∞–Ω –≤ —Ä–∞–º–∫–∞—Ö —á–∞—Å—Ç–∏—á–Ω–æ–π –º–æ–±–∏–ª–∏–∑–∞—Ü–∏–∏',
            '–ø—Ä–∏–∑–≤–∞–Ω –Ω–∞ –≤–æ–µ–Ω–Ω—É—é —Å–ª—É–∂–±—É –ø–æ –º–æ–±–∏–ª–∏–∑–∞—Ü–∏–∏',
            '–≤–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏–π –ø–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É –≤ –∑–æ–Ω–µ —Å–≤–æ',
            '—É—á–∞—Å—Ç–≤—É–µ—Ç –≤ —Å–≤–æ',
            '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∑–æ–Ω–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Å–≤–æ',
            '—Å—É–ø—Ä—É–≥ –¥–æ–ª–∂–Ω–∏–∫–∞ –Ω–∞ —Å–≤–æ',
            '–¥–æ–ª–∂–Ω–∏–∫ –Ω–∞ —Å–≤–æ',
            '–≤ —Å–≤—è–∑–∏ —Å –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ–º –Ω–∞ —Å–≤–æ',
            '–ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤ —Å–≤—è–∑–∏ —Å —Å–≤–æ',
            '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ä–∞–π–æ–Ω–µ —Å–≤–æ',
            # –ù–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –æ—Ç–ª–∞–¥–∫–∏
            '—è–≤–ª—è–µ—Ç—Å—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–º —Å–≤–æ',
            '—É—á–∞—Å—Ç–∏–µ–º –¥–æ–ª–∂–Ω–∏–∫–∞ –≤ —Å–≤–æ',
            '–Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ —Å–≤–æ',
            '–≤ –∑–æ–Ω–µ –¥–µ–π—Å—Ç–≤–∏—è —Å–≤–æ',
            '–±–æ–µ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ —Å–≤–æ',
            '–ø—Ä–∏–Ω–∏–º–∞–µ—Ç —É—á–∞—Å—Ç–∏–µ –≤ —Å–≤–æ',
            '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —Å–≤–æ',
            '—É—á–∞—Å—Ç–Ω–∏–∫ —Å–≤–æ –Ω–∞ —É–∫—Ä–∞–∏–Ω–µ',
            '–¥–æ–ª–∂–Ω–∏–∫ —è–≤–ª—è–µ—Ç—Å—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–º –°–í–û'
        ]

        for phrase in exact_phrases:
            if phrase in text_lower:
                print(f"‚úÖ –ù–ê–ô–î–ï–ù–ê –¢–û–ß–ù–ê–Ø –§–†–ê–ó–ê: {phrase}")
                return True

        # 2. –°–ª–æ–≤–æ "—Å–≤–æ" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (–±–æ–ª–µ–µ –≥–∏–±–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        words = re.findall(r'\b[–∞-—è—ë]+\b', text_lower)

        for i, word in enumerate(words):
            if word == '—Å–≤–æ':
                print(f"‚ö†Ô∏è –ù–ê–ô–î–ï–ù–û –°–õ–û–í–û '—Å–≤–æ' –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {i}")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫—Ä—É–∂–∞—é—â–∏–µ —Å–ª–æ–≤–∞ (–¥–æ 3 —Å–ª–æ–≤ –≤ –∫–∞–∂–¥—É—é —Å—Ç–æ—Ä–æ–Ω—É)
                start = max(0, i - 3)
                end = min(len(words), i + 4)
                context_words = words[start:end]
                context = ' '.join(context_words)
                print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ '—Å–≤–æ': {context}")

                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –°–í–û
                svo_patterns = [
                    '–Ω–∞ —Å–≤–æ',
                    '–≤ —Å–≤–æ',
                    '–∏–∑ —Å–≤–æ',
                    '—É—á–∞—Å—Ç–Ω–∏–∫ —Å–≤–æ',
                    '—Å—É–ø—Ä—É–≥ –Ω–∞ —Å–≤–æ',
                    '–¥–æ–ª–∂–Ω–∏–∫ –Ω–∞ —Å–≤–æ',
                    '–º–æ–±–∏–ª–∏–∑–æ–≤–∞–Ω –≤ —Å–≤–æ',
                    '–ø—Ä–∏–∑–≤–∞–Ω –≤ —Å–≤–æ',
                    '–≤–µ—Ä–Ω—É–ª—Å—è –∏–∑ —Å–≤–æ',
                    '–æ—Ç–ø—Ä–∞–≤–∏–ª—Å—è –≤ —Å–≤–æ',
                    '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∑–æ–Ω–µ —Å–≤–æ',
                    '—è–≤–ª—è–µ—Ç—Å—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–º —Å–≤–æ',  # –ù–æ–≤–æ–µ
                    '—É—á–∞—Å—Ç–∏–µ –≤ —Å–≤–æ',  # –ù–æ–≤–æ–µ
                    '—É—á–∞—Å—Ç–≤—É–µ—Ç –≤ —Å–≤–æ',  # –ù–æ–≤–æ–µ
                    '–∑–æ–Ω–µ —Å–≤–æ',  # –ù–æ–≤–æ–µ
                    '—Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ —Å–≤–æ'  # –ù–æ–≤–æ–µ
                ]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —ç—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
                for pattern in svo_patterns:
                    if pattern in context:
                        print(f"‚úÖ –ù–ê–ô–î–ï–ù –ü–ê–¢–¢–ï–†–ù: {pattern}")
                        return True

                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ "—Å–≤–æ" –µ—Å—Ç—å –∏ —Ä—è–¥–æ–º –Ω–µ—Ç –ª–æ–∂–Ω—ã—Ö —Å–ª–æ–≤
                false_context = ['—Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–æ', '—Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ', '—Å–≤–æ–¥–Ω–æ–µ', '—Å–≤–æ–¥–Ω—ã–π', '—Å–≤–æ–∏', '—Å–≤–æ–µ–π']
                if not any(fw in context for fw in false_context):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ "—Å–≤–æ" –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é –¥—Ä—É–≥–æ–≥–æ —Å–ª–æ–≤–∞
                    # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é "—Å–≤–æ" –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
                    svo_positions = [m.start() for m in re.finditer(r'—Å–≤–æ', text_lower)]
                    for pos in svo_positions:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–µ—Ä–µ–¥ "—Å–≤–æ" –Ω–µ –±—É–∫–≤–∞ –∏ –ø–æ—Å–ª–µ –Ω–µ –±—É–∫–≤–∞
                        if (pos == 0 or not text_lower[pos - 1].isalpha()) and \
                                (pos + 3 >= len(text_lower) or not text_lower[pos + 3].isalpha()):
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ "—Å–≤–æ–∏" –∏–ª–∏ "—Å–≤–æ–µ–π" –∏ —Ç.–¥.
                            next_chars = text_lower[pos:pos + 6]
                            if not any(next_chars.startswith(w) for w in ['—Å–≤–æ–∏', '—Å–≤–æ–µ–π', '—Å–≤–æ–µ–º', '—Å–≤–æ–∏–º']):
                                print(f"‚úÖ –ù–ê–ô–î–ï–ù–û –û–¢–î–ï–õ–¨–ù–û–ï –°–õ–û–í–û '—Å–≤–æ'")
                                return True

                print(f"‚ùå –ù–ï–¢ –ü–û–î–•–û–î–Ø–©–ï–ì–û –ü–ê–¢–¢–ï–†–ù–ê")

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É "–°–í–û" —Å –ø–æ—è—Å–Ω–µ–Ω–∏–µ–º
        if re.search(r'—Å–≤–æ\s*\([^)]*(–≤–æ–µ–Ω|–º–æ–±–∏–ª|—É—á–∞—Å—Ç|—Å–ø–µ—Ü|–æ–ø–µ—Ä–∞—Ü|—É–∫—Ä–∞–∏–Ω|–∑–æ–Ω)', text_lower):
            print(f"‚úÖ –ù–ê–ô–î–ï–ù–ê –ê–ë–ë–†–ï–í–ò–ê–¢–£–†–ê –í –°–ö–û–ë–ö–ê–•")
            return True

        # 4. –§—Ä–∞–∑—ã —Å–æ "—Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –≤–æ–µ–Ω–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è"
        if re.search(r'—Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è\s+–≤–æ–µ–Ω–Ω–∞—è\s+–æ–ø–µ—Ä–∞—Ü–∏—è', text_lower):
            print(f"‚úÖ –ù–ê–ô–î–ï–ù–ê –§–†–ê–ó–ê '—Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –≤–æ–µ–Ω–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è'")
            return True

        return False

    def _init_group_indexes(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≥—Ä—É–ø–ø"""
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self.groups_by_keyword = {}
        self.rejection_groups = {}  # –û—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –æ—Ç–∫–∞–∑–æ–≤
        self.non_rejection_groups = {}  # –î–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö

        for group in GROUP_VALUES:
            group_lower = group.lower()

            # 1. –î–ª—è –æ—Ç–∫–∞–∑–æ–≤ - —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ –ø—Ä–∏—á–∏–Ω–µ –æ—Ç–∫–∞–∑–∞
            if group_lower.startswith("–æ—Ç–∫–∞–∑"):
                if "," in group_lower:
                    _, reason = group_lower.split(",", 1)
                    reason = reason.strip()
                    # –£–±–∏—Ä–∞–µ–º "–æ—Ç–∫–∞–∑" –∏–∑ –Ω–∞—á–∞–ª–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    reason_clean = reason.replace("–æ—Ç–∫–∞–∑", "").strip()
                    if reason_clean:
                        if reason_clean not in self.rejection_groups:
                            self.rejection_groups[reason_clean] = []
                        self.rejection_groups[reason_clean].append(group)

                    # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—É—é –ø—Ä–∏—á–∏–Ω—É
                    if reason not in self.rejection_groups:
                        self.rejection_groups[reason] = []
                    self.rejection_groups[reason].append(group)
            else:
                # 2. –î–ª—è –Ω–µ-–æ—Ç–∫–∞–∑–æ–≤ - –∏–Ω–¥–µ–∫—Å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ (–¥–ª–∏–Ω–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
                words = [word.strip() for word in group_lower.replace(",", " ").split()
                         if len(word.strip()) > 3]

                for word in words:
                    if word not in self.non_rejection_groups:
                        self.non_rejection_groups[word] = []
                    self.non_rejection_groups[word].append(group)

        # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–º–µ—Ä—Ç–µ–π
        self.death_patterns = {
            "—É–º–µ—Ä": ["—É–º–µ—Ä", "—Å–∫–æ–Ω—á–∞–ª—Å—è", "–ø–æ–≥–∏–±", "—Å–º–µ—Ä—Ç–∏"],
            "—Å–º–µ—Ä—Ç—å": ["—Å–º–µ—Ä—Ç—å", "—Å–º–µ—Ä—Ç—å—é", "–ª–µ—Ç–∞–ª—å–Ω—ã–π –∏—Å—Ö–æ–¥"]
        }

        # –ì—Ä—É–ø–ø—ã –¥–ª—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏—è
        self.approval_patterns = ["—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å", "–∏—Å–ø–æ–ª–Ω–µ–Ω", "–ø—Ä–∏–Ω—è—Ç–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª–Ω–µ–Ω—ã"]

    @lru_cache(maxsize=10000)
    def _determine_group_cached(self, text: str) -> str:
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø—ã"""
        print(f"üì¶ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù –ö–≠–® –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {len(text)}")
        return self._determine_group_fast(text[:1000])

    def _determine_group_fast(self, text: str) -> str:
        print(f"‚ö° –í–´–ó–û–í _determine_group_fast –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {len(text)}")
        text_lower = text.lower()

        # 0. –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –°–í–û (—Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–≥–∞—è)
        if self._is_svo_detected(text):
            print(f"‚úÖ –û–ü–†–ï–î–ï–õ–ï–ù–û –ö–ê–ö –î–æ–ª–∂–Ω–∏–∫ –°–í–û")
            return "–î–æ–ª–∂–Ω–∏–∫ –°–í–û"

        # 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å
        if "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å" in text_lower:
            return next((g for g in GROUP_VALUES if "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å" in g.lower()), "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å")

        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–∫–∞–∑—ã
        if "–æ—Ç–∫–∞–∑" in text_lower:
            best_match = None
            best_score = 0

            for reason, groups in self.rejection_groups.items():
                reason_words = [w for w in reason.split() if len(w) > 3]
                if not reason_words:
                    continue

                matching_words = sum(1 for word in reason_words if word in text_lower)
                score = matching_words / len(reason_words)

                if score > best_score:
                    best_score = score
                    best_match = groups[0]

            if best_match and best_score >= 0.5:
                return best_match

        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–º–µ—Ä—Ç—å
        death_keywords = ["—É–º–µ—Ä", "—Å–º–µ—Ä—Ç—å", "—Å–∫–æ–Ω—á–∞–ª—Å—è", "–ø–æ–≥–∏–±", "—Å–º–µ—Ä—Ç–∏", "—Å–º–µ—Ä—Ç—å—é"]
        if any(keyword in text_lower for keyword in death_keywords):
            for group in GROUP_VALUES:
                if any(keyword in group.lower() for keyword in death_keywords):
                    return group

        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—Ä—É–≥–∏–µ –≥—Ä—É–ø–ø—ã
        text_words = [w for w in text_lower.split() if len(w) > 3]

        for word in text_words:
            if word in self.non_rejection_groups:
                for group in self.non_rejection_groups[word]:
                    group_words = [w for w in group.lower().split() if len(w) > 3]
                    matches = sum(1 for gw in group_words if gw in text_lower)
                    if matches >= max(1, len(group_words) * 0.7):
                        return group

        # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if any(phrase in text_lower for phrase in ["–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –æ–∫–æ–Ω—á–µ–Ω–æ", "–∏–ø –æ–∫–æ–Ω—á–µ–Ω–æ", "–æ–∫–æ–Ω—á–∞–Ω–∏–∏ –∏–ø"]):
            for group in GROUP_VALUES:
                if "–æ–∫–æ–Ω—á–µ–Ω–æ" in group.lower():
                    return group

        if any(phrase in text_lower for phrase in ["–∏—Å–ø–æ–ª–Ω–µ–Ω", "—Ä–∞–Ω–µ–µ –∏—Å–ø–æ–ª–Ω–µ–Ω", "—Ä–∞–Ω–µ–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω"]):
            for group in GROUP_VALUES:
                if "—Ä–∞–Ω–µ–µ" in group.lower():
                    return group

        if any(phrase in text_lower for phrase in ["–Ω–µ —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–µ–Ω", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–±–µ–∑—Ä–∞–±–æ—Ç–Ω—ã–π"]):
            for group in GROUP_VALUES:
                if "—Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–µ–Ω" in group.lower():
                    return group

        return "—Å–º–æ—Ç—Ä–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç"

    def _determine_group(self, text: str) -> str:
        """–ë—ã—Å—Ç—Ä–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞"""
        return self._determine_group_cached(text[:1000])

    def _is_death_group(self, group: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —É–∫–∞–∑—ã–≤–∞–µ—Ç –ª–∏ –≥—Ä—É–ø–ø–∞ –Ω–∞ —Å–º–µ—Ä—Ç—å –¥–æ–ª–∂–Ω–∏–∫–∞"""
        if not group:
            return False

        group_lower = group.lower()
        death_keywords = ["—É–º–µ—Ä", "—Å–º–µ—Ä—Ç—å", "—Å–º–µ—Ä—Ç–∏", "—Å–∫–æ–Ω—á–∞–ª—Å—è", "–ø–æ–≥–∏–±", "–ª–µ—Ç–∞–ª—å–Ω—ã–π"]

        for keyword in death_keywords:
            if keyword in group_lower:
                return True

        return group in DEATH_GROUPS

    def detect_death_with_neural_network(self, text: str) -> bool:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–º–µ—Ä—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if self.death_neural_network:
            return self.death_neural_network.detect_death(text)
        else:
            # –ï—Å–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
            text_lower = text.lower()
            death_keywords = ["—É–º–µ—Ä", "—Å–º–µ—Ä—Ç—å", "—Å–∫–æ–Ω—á–∞–ª—Å—è", "–ø–æ–≥–∏–±", "—Å–º–µ—Ä—Ç–∏", "—Å–º–µ—Ä—Ç—å—é"]
            return any(keyword in text_lower for keyword in death_keywords)

    def analyze_motivation_resolution_fast(self, motivation_text: str, resolution_text: str) -> Dict[str, Any]:
        """–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –í–ê–†–ò–ê–ù–¢"""
        combined_text = f"{motivation_text} {resolution_text}"
        combined_text_lower = combined_text.lower()

        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
        if len(combined_text) < 50:
            return {
                '—Ä–µ–∑—É–ª—å—Ç–∞—Ç': '–ó–∞—è–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–æ',
                '–≥—Ä—É–ø–ø–∞': '—Å–º–æ—Ç—Ä–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç',
                '—Å–º–µ—Ä—Ç—å_–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞': False,
                '–ø—Ä–∏–∑–Ω–∞–∫_—Å–º–µ—Ä—Ç–∏_–∏–∏': False
            }

        # 0. –ü–†–û–í–ï–†–ö–ê –°–õ–û–í-–ò–°–ö–õ–Æ–ß–ï–ù–ò–ô (–í–´–°–®–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢)
        if contains_exception_word(combined_text):
            print(f"‚úÖ –ù–ê–ô–î–ï–ù–´ –°–õ–û–í–ê-–ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø –í –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ï - –¥–æ–ª–∂–Ω–∏–∫ –ñ–ò–í")
            return {
                '—Ä–µ–∑—É–ª—å—Ç–∞—Ç': '–î–æ–ª–∂–Ω–∏–∫ –∂–∏–≤ (–Ω–∞–π–¥–µ–Ω—ã —Å–ª–æ–≤–∞-–∏—Å–∫–ª—é—á–µ–Ω–∏—è)',
                '–≥—Ä—É–ø–ø–∞': '–î–æ–ª–∂–Ω–∏–∫ –∂–∏–≤',
                '—Å–º–µ—Ä—Ç—å_–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞': False,
                '—Å–º–µ—Ä—Ç—å_–≤_—Ç–µ–∫—Å—Ç–µ': False,
                '—Å–º–µ—Ä—Ç—å_–≤_–≥—Ä—É–ø–ø–µ': False,
                '–ø—Ä–∏–∑–Ω–∞–∫_—Å–º–µ—Ä—Ç–∏_–∏–∏': False
            }

        # 0. –ü–†–û–í–ï–†–ö–ê –°–¢–û–ü-–§–†–ê–ó (–í–´–°–®–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢)
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ª—é–±–∞—è —Å—Ç–æ–ø-—Ñ—Ä–∞–∑–∞ - –¥–æ–ª–∂–Ω–∏–∫ –ñ–ò–í
        for phrase in STOP_PHRASES:
            if phrase in combined_text_lower:
                print(f"‚úÖ –ù–ê–ô–î–ï–ù–ê –°–¢–û–ü-–§–†–ê–ó–ê –í –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ï: {phrase}")
                return {
                    '—Ä–µ–∑—É–ª—å—Ç–∞—Ç': '–î–æ–ª–∂–Ω–∏–∫ –∂–∏–≤ (–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑–æ–π)',
                    '–≥—Ä—É–ø–ø–∞': '–î–æ–ª–∂–Ω–∏–∫ –∂–∏–≤',
                    '—Å–º–µ—Ä—Ç—å_–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞': False,
                    '—Å–º–µ—Ä—Ç—å_–≤_—Ç–µ–∫—Å—Ç–µ': False,
                    '—Å–º–µ—Ä—Ç—å_–≤_–≥—Ä—É–ø–ø–µ': False,
                    '–ø—Ä–∏–∑–Ω–∞–∫_—Å–º–µ—Ä—Ç–∏_–∏–∏': False
                }

        # 1. –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–º–µ—Ä—Ç–∏ –ø–æ —Ç–µ–∫—Å—Ç—É
        death_detected_text = bool(self.death_pattern.search(combined_text))

        # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É
        print(f"\nüîç –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ì–†–£–ü–ü–´ –î–õ–Ø –ó–ê–Ø–í–ö–ò")
        print(f"–¢–ï–ö–°–¢ (–ø–µ—Ä–≤—ã–µ 200): {combined_text[:200]}")
        group = self._determine_group(combined_text)
        print(f"–û–ü–†–ï–î–ï–õ–ï–ù–ê –ì–†–£–ü–ü–ê: {group}")

        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–º–µ—Ä—Ç—å –ø–æ –≥—Ä—É–ø–ø–µ
        death_detected_group = self._is_death_group(group)

        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–º–µ—Ä—Ç—å —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
        print(f"üß† –ó–ê–ü–£–°–ö –ù–ï–ô–†–û–°–ï–¢–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê –°–ú–ï–†–¢–ò")
        death_detected_neural = self.detect_death_with_neural_network(combined_text)
        print(f"üß† –†–ï–ó–£–õ–¨–¢–ê–¢ –ù–ï–ô–†–û–°–ï–¢–ò: {death_detected_neural}")

        # 5. –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–º–µ—Ä—Ç–∏
        death_detected = death_detected_text or death_detected_group or death_detected_neural

        # 6. –ë—ã—Å—Ç—Ä–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        if death_detected:
            summary = self._find_death_context(combined_text)
        else:
            if resolution_text and len(resolution_text) > 30:
                summary = self._extractive_summarize_fast(resolution_text[:600])
            elif motivation_text and len(motivation_text) > 30:
                summary = self._extractive_summarize_fast(motivation_text[:600])
            else:
                summary = "–ó–∞—è–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–æ"

        # 7. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result_text = summary
        if group != "—Å–º–æ—Ç—Ä–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç" and group not in summary:
            result_text = f"{summary} ({group})"

        return {
            '—Ä–µ–∑—É–ª—å—Ç–∞—Ç': result_text[:300],
            '–≥—Ä—É–ø–ø–∞': group,
            '—Å–º–µ—Ä—Ç—å_–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞': death_detected,
            '—Å–º–µ—Ä—Ç—å_–≤_—Ç–µ–∫—Å—Ç–µ': death_detected_text,
            '—Å–º–µ—Ä—Ç—å_–≤_–≥—Ä—É–ø–ø–µ': death_detected_group,
            '–ø—Ä–∏–∑–Ω–∞–∫_—Å–º–µ—Ä—Ç–∏_–∏–∏': death_detected_neural  # –ù–æ–≤–æ–µ –ø–æ–ª–µ –æ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
        }

    def _extractive_summarize_fast(self, text: str) -> str:
        """–ë—ã—Å—Ç—Ä–∞—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"""
        if not text:
            return ""

        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 15]

        if not sentences:
            return text[:80] + "..." if len(text) > 80 else text

        important_sentences = []
        legal_keywords = ['—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å', '–æ—Ç–∫–∞–∑–∞—Ç—å', '–ø–æ—Å—Ç–∞–Ω–æ–≤–∏–ª', '—Ä–µ—à–µ–Ω–∏–µ', '–¥–æ–ª–∂–Ω–∏–∫', '–≤–∑—ã—Å–∫–∞—Ç–µ–ª—å']

        for sentence in sentences:
            score = 0
            sentence_lower = sentence.lower()

            for keyword in legal_keywords:
                if keyword in sentence_lower:
                    score += 2

            if self.number_pattern.search(sentence):
                score += 3

            if score >= 2:
                important_sentences.append(sentence)
                if len(important_sentences) >= 2:
                    break

        if important_sentences:
            summary = '. '.join(important_sentences[:2]) + '.'
        else:
            summary = sentences[0]

        if len(summary) > 200:
            summary = summary[:197] + "..."

        return summary

    def _find_death_context(self, text: str) -> str:
        """–ù–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–º–µ—Ä—Ç–∏"""
        match = self.death_pattern.search(text.lower())
        if match:
            start = max(0, match.start() - 50)
            end = min(len(text), match.end() + 50)
            context = text[start:end].strip()
            return f"–°–º–µ—Ä—Ç—å: {context}"
        return "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å–º–µ—Ä—Ç—å –¥–æ–ª–∂–Ω–∏–∫–∞"


class FastCollector:
    """–ë—ã—Å—Ç—Ä—ã–π –∫–æ–ª–ª–µ–∫—Ç–æ—Ä - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô"""

    def __init__(self, max_workers_per_domain: int = 16):
        self.base_domain = 'https://main.techlegal.ru'
        self.resource = 'api'
        self.token = ""
        self.max_workers_per_domain = max_workers_per_domain

        try:
            if TRANSFORMERS_AVAILABLE:
                self.analyzer = FinalTransformersAnalyzer()
                print("‚úÖ Transformers –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            else:
                raise ImportError("Transformers –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}")
            raise

        # –ï–¥–∏–Ω—ã–π JSONL —Ñ–∞–π–ª
        self.jsonl_file = None
        self.jsonl_lock = threading.Lock()

        # –ü–∞–∫–µ—Ç–Ω–∞—è –∑–∞–ø–∏—Å—å
        self.write_buffer = []
        self.buffer_size = 100
        self.buffer_lock = threading.Lock()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_domains': 0,
            'domains_processed': 0,
            'domains_failed': 0,
            'total_orders': 0,
            'death_cases': 0,
            'death_by_text': 0,
            'death_by_group': 0,
            'death_by_neural': 0,  # –ù–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            'exception_words_found': 0,  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º-–∏—Å–∫–ª—é—á–µ–Ω–∏—è–º
            'svo_cases': 0,
            'start_time': datetime.now(),
            'processed_per_second': 0,
            'last_update': datetime.now(),
            'last_count': 0
        }
        self.stats_lock = threading.Lock()

        os.makedirs('data/final', exist_ok=True)

        print(f"‚úÖ –ë—ã—Å—Ç—Ä—ã–π –∫–æ–ª–ª–µ–∫—Ç–æ—Ä –≥–æ—Ç–æ–≤ (–ø–æ—Ç–æ–∫–æ–≤: {max_workers_per_domain})")
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(EXCEPTION_WORDS)} —Å–ª–æ–≤-–∏—Å–∫–ª—é—á–µ–Ω–∏–π")

    def create_client(self):
        """–°–æ–∑–¥–∞—Ç—å HTTP –∫–ª–∏–µ–Ω—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=100,
            pool_maxsize=100,
            max_retries=3
        )
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        session.timeout = 15
        return session

    def open_jsonl_file(self):
        """–û—Ç–∫—Ä—ã—Ç—å –µ–¥–∏–Ω—ã–π JSONL —Ñ–∞–π–ª"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.jsonl_filename = f"data/final/–∑–∞—è–≤–∫–∏_–≤—Å–µ_–¥–æ–º–µ–Ω—ã_{timestamp}.jsonl"

        try:
            self.jsonl_file = open(self.jsonl_filename, 'w', encoding='utf-8', buffering=32768)
            print(f"üìÅ –°–æ–∑–¥–∞–Ω –µ–¥–∏–Ω—ã–π JSONL —Ñ–∞–π–ª: {self.jsonl_filename}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            return False

    def close_jsonl_file(self):
        """–ó–∞–∫—Ä—ã—Ç—å JSONL —Ñ–∞–π–ª"""
        if self.jsonl_file:
            self._flush_buffer()
            try:
                self.jsonl_file.close()
                file_size = os.path.getsize(self.jsonl_filename) / (1024 * 1024)
                print(f"üìÅ –ó–∞–∫—Ä—ã—Ç JSONL —Ñ–∞–π–ª: {self.jsonl_filename}")
                print(f"üíæ –†–∞–∑–º–µ—Ä: {file_size:.2f} MB")
                print(f"üìä –ó–∞–ø–∏—Å–µ–π: {self.stats['total_orders']:,}")
            except Exception as e:
                print(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è —Ñ–∞–π–ª–∞: {e}")

    def _flush_buffer(self):
        """–ó–∞–ø–∏—Å–∞—Ç—å –±—É—Ñ–µ—Ä –≤ —Ñ–∞–π–ª"""
        with self.buffer_lock:
            if not self.write_buffer:
                return

            try:
                with self.jsonl_lock:
                    for appeal in self.write_buffer:
                        json_line = json.dumps(appeal, ensure_ascii=False)
                        self.jsonl_file.write(json_line + '\n')
                    self.jsonl_file.flush()
                self.write_buffer.clear()
            except Exception as e:
                print(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –±—É—Ñ–µ—Ä–∞: {e}")

    def save_to_buffer(self, appeal: Dict):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–∞—è–≤–∫—É –≤ –±—É—Ñ–µ—Ä"""
        with self.buffer_lock:
            self.write_buffer.append(appeal)
            if len(self.write_buffer) >= self.buffer_size:
                threading.Thread(target=self._flush_buffer, daemon=True).start()

    def get_all_orders_fast(self, client, domain: str) -> tuple:
        """–ë—ã—Å—Ç—Ä–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞—è–≤–æ–∫ —Å –¥–æ–º–µ–Ω–∞"""
        print(f"    üì° –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å {domain}")

        all_orders = []
        offset = 0
        page_size = 10000
        max_pages = 50
        empty_pages = 0

        for page in range(1, max_pages + 1):
            success, orders = self._get_page_fast(client, domain, offset, page_size)

            if not success:
                print(f"    ‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                break

            if orders:
                all_orders.extend(orders)
                print(f"    üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: +{len(orders):,} –∑–∞—è–≤–æ–∫")

                if len(orders) < page_size:
                    print(f"    ‚úÖ –ü–æ–ª—É—á–µ–Ω—ã –≤—Å–µ –¥–∞–Ω–Ω—ã–µ")
                    break

                empty_pages = 0
            else:
                empty_pages += 1
                if empty_pages >= 3:
                    print(f"    ‚úÖ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –Ω–∞ {empty_pages} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö –ø–æ–¥—Ä—è–¥")
                    break

            offset += page_size
            if page % 5 == 0:
                time.sleep(0.05)

        print(f"    ‚úÖ –ò—Ç–æ–≥–æ —Å –¥–æ–º–µ–Ω–∞: {len(all_orders):,} –∑–∞—è–≤–æ–∫")
        return True, all_orders

    def _get_page_fast(self, client, domain: str, offset: int, count: int) -> tuple:
        """–ë—ã—Å—Ç—Ä–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        url = f'{domain.rstrip("/")}/{self.resource}/getRequestFsspResponse'
        payload = {
            'token': self.token,
            'count': count,
            'isSqueezeText': 1,
            'offset': offset
        }

        try:
            response = client.post(url, json=payload, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if isinstance(data, list):
                    return True, data
                return True, []
            return False, []
        except Exception as e:
            print(f"    ‚ö† –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return False, []

    def process_request_fast(self, request_data: Dict, domain: str) -> Optional[Dict]:
        """–ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∑–∞—è–≤–∫–∏ - –ü–û–õ–ù–´–ï –¢–ï–ö–°–¢–´ –ë–ï–ó –°–û–ö–†–ê–©–ï–ù–ò–ô"""
        try:
            motivation = str(request_data.get('text', ''))
            resolution = str(request_data.get('textResolution', ''))

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–æ–≤–∞-–∏—Å–∫–ª—é—á–µ–Ω–∏—è
            combined_text = f"{motivation} {resolution}"
            has_exception = contains_exception_word(combined_text)

            if has_exception:
                with self.stats_lock:
                    self.stats['exception_words_found'] += 1

            # –ê–Ω–∞–ª–∏–∑ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ—Ç–∏–≤–∞—Ü–∏—é –∏ —Ä–µ–∑–æ–ª—é—Ü–∏—é)
            analysis = self.analyzer.analyze_motivation_resolution_fast(motivation, resolution)

            # ID –∑–∞—è–≤–∫–∏
            request_id = str(request_data.get('requestId', ''))
            if not request_id or request_id.lower() in ['null', 'none', '0']:
                number = str(request_data.get('number', ''))[:20]
                text_hash = hashlib.md5(str(request_data.get('text', '')).encode()).hexdigest()[:8]
                request_id = f"{domain[:10]}_{number}_{text_hash}"

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –∏–∑ motivation + resolution
            full_response_text = f"{motivation}\n\n{resolution}".strip()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            full_response_lower = full_response_text.lower()
            has_stop_phrase = any(phrase in full_response_lower for phrase in STOP_PHRASES)

            if has_stop_phrase:
                found_phrases = [phrase for phrase in STOP_PHRASES if phrase in full_response_lower]
                print(f"üîç –ó–ê–Ø–í–ö–ê –°–û –°–¢–û–ü-–§–†–ê–ó–û–ô: {request_id[:30]}... - {', '.join(found_phrases[:2])}")

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å - –ü–û–õ–ù–´–ï –¢–ï–ö–°–¢–´ –ë–ï–ó –°–û–ö–†–ê–©–ï–ù–ò–ô
            processed = {
                'id_–∑–∞—è–≤–∫–∏': request_id,
                '–Ω–∞–∑–≤–∞–Ω–∏–µ_–∑–∞—è–≤–ª–µ–Ω–∏—è': request_data.get('typeAppeal', '–Ω–µ —É–∫–∞–∑–∞–Ω'),
                '—Ç–µ–∫—Å—Ç_–∑–∞—è–≤–ª–µ–Ω–∏—è': request_data.get('text', ''),
                '—Ç–µ–∫—Å—Ç_–æ—Ç–≤–µ—Ç–∞': full_response_text,  # –ü–û–õ–ù–´–ô —Ç–µ–∫—Å—Ç motivation + resolution
                '—Ä–µ–∑—É–ª—å—Ç–∞—Ç': analysis['—Ä–µ–∑—É–ª—å—Ç–∞—Ç'],
                '–≥—Ä—É–ø–ø–∞': analysis['–≥—Ä—É–ø–ø–∞'],
                '—Å–º–µ—Ä—Ç—å_–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞': analysis['—Å–º–µ—Ä—Ç—å_–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞'],
                '—Å–º–µ—Ä—Ç—å_–≤_—Ç–µ–∫—Å—Ç–µ': analysis.get('—Å–º–µ—Ä—Ç—å_–≤_—Ç–µ–∫—Å—Ç–µ', False),
                '—Å–º–µ—Ä—Ç—å_–≤_–≥—Ä—É–ø–ø–µ': analysis.get('—Å–º–µ—Ä—Ç—å_–≤_–≥—Ä—É–ø–ø–µ', False),
                '–ø—Ä–∏–∑–Ω–∞–∫_—Å–º–µ—Ä—Ç–∏_–∏–∏': analysis.get('–ø—Ä–∏–∑–Ω–∞–∫_—Å–º–µ—Ä—Ç–∏_–∏–∏', False),  # –ù–æ–≤–æ–µ –ø–æ–ª–µ –æ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
                '—Å–ª–æ–≤–∞_–∏—Å–∫–ª—é—á–µ–Ω–∏—è_–Ω–∞–π–¥–µ–Ω—ã': has_exception,  # –ü–æ–ª–µ –¥–ª—è —Å–ª–æ–≤-–∏—Å–∫–ª—é—á–µ–Ω–∏–π
                '–¥–æ–º–µ–Ω': domain[:50]
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±—É—Ñ–µ—Ä
            self.save_to_buffer(processed)

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            with self.stats_lock:
                self.stats['total_orders'] += 1
                if analysis['—Å–º–µ—Ä—Ç—å_–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞']:
                    self.stats['death_cases'] += 1
                if analysis.get('—Å–º–µ—Ä—Ç—å_–≤_—Ç–µ–∫—Å—Ç–µ', False):
                    self.stats['death_by_text'] += 1
                if analysis.get('—Å–º–µ—Ä—Ç—å_–≤_–≥—Ä—É–ø–ø–µ', False):
                    self.stats['death_by_group'] += 1
                if analysis.get('–ø—Ä–∏–∑–Ω–∞–∫_—Å–º–µ—Ä—Ç–∏_–∏–∏', False):  # –ù–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    self.stats['death_by_neural'] += 1
                if analysis['–≥—Ä—É–ø–ø–∞'] == "–î–æ–ª–∂–Ω–∏–∫ –°–í–û":
                    self.stats['svo_cases'] += 1

            return processed

        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞—è–≤–∫–∏: {e}")
            return None

    def process_domain_fast(self, domain: str) -> Dict:
        """–ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–º–µ–Ω–∞"""
        client = self.create_client()
        domain_stats = {
            'domain': domain,
            'orders_received': 0,
            'orders_processed': 0,
            'death_cases': 0,
            'death_by_text': 0,
            'death_by_group': 0,
            'death_by_neural': 0,  # –ù–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            'exception_words_found': 0,  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º-–∏—Å–∫–ª—é—á–µ–Ω–∏—è–º
            'svo_cases': 0,
            'error': None,
            'start_time': datetime.now()
        }

        try:
            print(f"\nüåê –ù–∞—á–∏–Ω–∞–µ–º –¥–æ–º–µ–Ω: {domain}")

            success, all_orders = self.get_all_orders_fast(client, domain)

            if not success or not all_orders:
                domain_stats['error'] = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
                return domain_stats

            domain_stats['orders_received'] = len(all_orders)

            print(f"    üöÄ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(all_orders):,} –∑–∞—è–≤–æ–∫...")

            batch_size = 500
            total_batches = (len(all_orders) + batch_size - 1) // batch_size

            for batch_num in range(total_batches):
                start_idx = batch_num * batch_size
                end_idx = min((batch_num + 1) * batch_size, len(all_orders))
                batch = all_orders[start_idx:end_idx]

                with ThreadPoolExecutor(max_workers=self.max_workers_per_domain) as executor:
                    futures = []
                    for order in batch:
                        future = executor.submit(self.process_request_fast, order, domain)
                        futures.append(future)

                    for future in as_completed(futures):
                        try:
                            result = future.result(timeout=5)
                            if result:
                                domain_stats['orders_processed'] += 1
                                if result.get('—Å–º–µ—Ä—Ç—å_–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞'):
                                    domain_stats['death_cases'] += 1
                                    if result.get('—Å–º–µ—Ä—Ç—å_–≤_—Ç–µ–∫—Å—Ç–µ'):
                                        domain_stats['death_by_text'] += 1
                                    if result.get('—Å–º–µ—Ä—Ç—å_–≤_–≥—Ä—É–ø–ø–µ'):
                                        domain_stats['death_by_group'] += 1
                                    if result.get('–ø—Ä–∏–∑–Ω–∞–∫_—Å–º–µ—Ä—Ç–∏_–∏–∏'):  # –ù–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                                        domain_stats['death_by_neural'] += 1
                                if result.get('—Å–ª–æ–≤–∞_–∏—Å–∫–ª—é—á–µ–Ω–∏—è_–Ω–∞–π–¥–µ–Ω—ã'):
                                    domain_stats['exception_words_found'] += 1
                                if result.get('–≥—Ä—É–ø–ø–∞') == "–î–æ–ª–∂–Ω–∏–∫ –°–í–û":
                                    domain_stats['svo_cases'] += 1
                        except:
                            pass

                if (batch_num + 1) % 5 == 0 or batch_num == total_batches - 1:
                    progress = (batch_num + 1) / total_batches * 100
                    print(f"    üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {batch_num + 1}/{total_batches} –ø–∞–∫–µ—Ç–æ–≤ ({progress:.1f}%)")

            time_spent = datetime.now() - domain_stats['start_time']
            speed = domain_stats[
                        'orders_processed'] / time_spent.total_seconds() if time_spent.total_seconds() > 0 else 0

            print(f"    ‚úÖ –î–æ–º–µ–Ω –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {time_spent}")
            print(f"       üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {domain_stats['orders_processed']:,}")
            print(f"       ‚ö∞Ô∏è  –°–º–µ—Ä—Ç–µ–π: {domain_stats['death_cases']:,}")
            print(f"       üìù –ü–æ —Ç–µ–∫—Å—Ç—É: {domain_stats['death_by_text']:,}")
            print(f"       üè∑Ô∏è  –ü–æ –≥—Ä—É–ø–ø–µ: {domain_stats['death_by_group']:,}")
            print(f"       üß† –ü–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {domain_stats['death_by_neural']:,}")
            print(f"       üö´ –°–ª–æ–≤-–∏—Å–∫–ª—é—á–µ–Ω–∏–π: {domain_stats['exception_words_found']:,}")
            print(f"       ü™ñ –î–æ–ª–∂–Ω–∏–∫ –°–í–û: {domain_stats['svo_cases']:,}")
            print(f"       ‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {speed:.1f} –∑–∞—è–≤–æ–∫/—Å–µ–∫")

            return domain_stats

        except Exception as e:
            domain_stats['error'] = str(e)
            print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
            return domain_stats
        finally:
            client.close()
            gc.collect()

    def collect_all_domains_fast(self, domains: List[str]) -> bool:
        """–°–±–æ—Ä –≤—Å–µ—Ö –¥–æ–º–µ–Ω–æ–≤ –≤ –æ–¥–∏–Ω JSONL - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π"""
        print(f"\nüöÄ –ó–ê–ü–£–°–ö –ë–´–°–¢–†–û–ì–û –°–ë–û–†–ê")
        print(f"üåê –î–æ–º–µ–Ω–æ–≤: {len(domains):,}")
        print(f"üìÅ –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: –û–î–ò–ù JSONL")
        print(f"üìÑ –¢–µ–∫—Å—Ç –∑–∞—è–≤–ª–µ–Ω–∏—è: –ü–û–õ–ù–´–ô –∏–∑ fileTextName")
        print(f"üìÑ –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞: –ü–û–õ–ù–´–ô (motivation + resolution)")
        print(f"üß† –ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è —Å–º–µ—Ä—Ç–∏: cointegrated/rubert-tiny2-cedr-emotion-detection")
        print(f"üö´ –°–ª–æ–≤-–∏—Å–∫–ª—é—á–µ–Ω–∏–π: {len(EXCEPTION_WORDS)}")
        print(f"üöÄ –ü–æ—Ç–æ–∫–æ–≤ –Ω–∞ –¥–æ–º–µ–Ω: {self.max_workers_per_domain}")
        print("=" * 80)

        if not self.open_jsonl_file():
            return False

        try:
            start_time = datetime.now()

            for i, domain in enumerate(domains, 1):
                print(f"\n[{i}/{len(domains)}] ", end="")
                result = self.process_domain_fast(domain)

                if result['error']:
                    with self.stats_lock:
                        self.stats['domains_failed'] += 1
                else:
                    with self.stats_lock:
                        self.stats['domains_processed'] += 1

                self._show_progress(i, len(domains), start_time)

            self.close_jsonl_file()
            self._save_final_statistics()
            return True

        except KeyboardInterrupt:
            print(f"\n\n‚ö†Ô∏è  –ü–†–ï–†–í–ê–ù–û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ú")
            self.close_jsonl_file()
            return False
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            self.close_jsonl_file()
            return False

    def _show_progress(self, current: int, total: int, start_time: datetime):
        """–ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å"""
        elapsed = datetime.now() - start_time
        progress = current / total * 100

        with self.stats_lock:
            speed = self.stats['total_orders'] / elapsed.total_seconds() if elapsed.total_seconds() > 0 else 0

            print(f"\nüìà –û–ë–©–ò–ô –ü–†–û–ì–†–ï–°–°:")
            print(f"   –î–æ–º–µ–Ω–æ–≤: {current}/{total} ({progress:.1f}%)")
            print(f"   –ó–∞—è–≤–æ–∫: {self.stats['total_orders']:,}")
            print(f"   ‚ö∞Ô∏è  –°–º–µ—Ä—Ç–µ–π: {self.stats['death_cases']:,}")
            print(f"      –ü–æ —Ç–µ–∫—Å—Ç—É: {self.stats['death_by_text']:,}")
            print(f"      –ü–æ –≥—Ä—É–ø–ø–µ: {self.stats['death_by_group']:,}")
            print(f"      üß† –ü–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {self.stats['death_by_neural']:,}")
            print(f"   üö´ –°–ª–æ–≤-–∏—Å–∫–ª—é—á–µ–Ω–∏–π: {self.stats['exception_words_found']:,}")
            print(f"   ü™ñ –î–æ–ª–∂–Ω–∏–∫ –°–í–û: {self.stats['svo_cases']:,}")
            print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {speed:.1f} –∑–∞—è–≤–æ–∫/—Å–µ–∫")
            print(f"   –ü—Ä–æ—à–ª–æ –≤—Ä–µ–º–µ–Ω–∏: {elapsed}")

    def _save_final_statistics(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        try:
            stats_file = f"data/final/—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞_—Ñ–∏–Ω–∞–ª_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            with self.stats_lock:
                stats = {
                    '–≤—Å–µ–≥–æ_–¥–æ–º–µ–Ω–æ–≤': self.stats['domains_processed'] + self.stats['domains_failed'],
                    '–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ_–¥–æ–º–µ–Ω–æ–≤': self.stats['domains_processed'],
                    '–æ—à–∏–±–æ–∫_–¥–æ–º–µ–Ω–æ–≤': self.stats['domains_failed'],
                    '–≤—Å–µ–≥–æ_–∑–∞—è–≤–æ–∫': self.stats['total_orders'],
                    '—Å–º–µ—Ä—Ç–µ–π': self.stats['death_cases'],
                    '—Å–º–µ—Ä—Ç–µ–π_–ø–æ_—Ç–µ–∫—Å—Ç—É': self.stats['death_by_text'],
                    '—Å–º–µ—Ä—Ç–µ–π_–ø–æ_–≥—Ä—É–ø–ø–µ': self.stats['death_by_group'],
                    '—Å–º–µ—Ä—Ç–µ–π_–ø–æ_–Ω–µ–π—Ä–æ—Å–µ—Ç–∏': self.stats['death_by_neural'],
                    '—Å–ª–æ–≤_–∏—Å–∫–ª—é—á–µ–Ω–∏–π_–Ω–∞–π–¥–µ–Ω–æ': self.stats['exception_words_found'],
                    '–¥–æ–ª–∂–Ω–∏–∫_–°–í–û': self.stats['svo_cases'],
                    '–≥—Ä—É–ø–ø—ã_—Å–º–µ—Ä—Ç–∏': list(DEATH_GROUPS),
                    '—Å—Ç–æ–ø_—Ñ—Ä–∞–∑—ã': STOP_PHRASES,
                    '—Å–ª–æ–≤–∞_–∏—Å–∫–ª—é—á–µ–Ω–∏—è': EXCEPTION_WORDS[:50],  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ 50 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                    '–≤—Ä–µ–º—è_–Ω–∞—á–∞–ª–∞': self.stats['start_time'].isoformat(),
                    '–≤—Ä–µ–º—è_–æ–∫–æ–Ω—á–∞–Ω–∏—è': datetime.now().isoformat(),
                    '–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å': str(datetime.now() - self.stats['start_time']),
                    'jsonl_—Ñ–∞–π–ª': self.jsonl_filename
                }

            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)

            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {stats_file}")

        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")

    def get_all_domains(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–æ–º–µ–Ω—ã"""
        client = self.create_client()
        url = f'{self.base_domain}/{self.resource}/getRequestFsspResponseCountDomain'
        payload = {'token': self.token}

        try:
            response = client.post(url, json=payload, timeout=30)
            if response.status_code == 200:
                all_domains = [el['domain'] for el in response.json()]
                domains = [domain for domain in all_domains if domain not in EXCLUDED_DOMAINS]

                print(f'‚úÖ –ü–æ–ª—É—á–µ–Ω–æ –¥–æ–º–µ–Ω–æ–≤: {len(all_domains)}')
                print(f'üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(domains)}')

                if len(all_domains) - len(domains) > 0:
                    print(f'üö´ –ò—Å–∫–ª—é—á–µ–Ω–æ: {len(all_domains) - len(domains)} –¥–æ–º–µ–Ω–æ–≤')

                return domains

        except Exception as e:
            print(f'‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–º–µ–Ω–æ–≤: {e}')
        finally:
            client.close()

        return []


def main():
    print("=" * 80)
    print("üöÄ –ë–´–°–¢–†–´–ô TRANSFORMERS-–ê–ù–ê–õ–ò–ó –° –ú–ù–û–ì–û–ü–û–¢–û–ß–ù–û–°–¢–¨–Æ")
    print("=" * 80)
    print("üîç –û–ë–ù–û–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –≥—Ä—É–ø–ø–∞ '–î–æ–ª–∂–Ω–∏–∫ –°–í–û'")
    print("üß† –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–º–µ—Ä—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Å—Ç–æ–ª–±—Ü–µ")
    print("   –ú–æ–¥–µ–ª—å: cointegrated/rubert-tiny2-cedr-emotion-detection")
    print("üìÑ –¢–µ–∫—Å—Ç –∑–∞—è–≤–ª–µ–Ω–∏—è: –ü–û–õ–ù–´–ô –∏–∑ fileTextName")
    print("üìÑ –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞: –ü–û–õ–ù–´–ô (motivation + resolution)")
    print("üìä –ù–æ–≤–æ–µ –ø–æ–ª–µ: –ø—Ä–∏–∑–Ω–∞–∫_—Å–º–µ—Ä—Ç–∏_–∏–∏ (True/False)")
    print("üö´ –ù–æ–≤–æ–µ –ø–æ–ª–µ: —Å–ª–æ–≤–∞_–∏—Å–∫–ª—é—á–µ–Ω–∏—è_–Ω–∞–π–¥–µ–Ω—ã (True/False)")
    print(f"üö´ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(EXCEPTION_WORDS)} —Å–ª–æ–≤-–∏—Å–∫–ª—é—á–µ–Ω–∏–π")
    print("üõë –î–æ–±–∞–≤–ª–µ–Ω—ã —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã: –ø—Ä–∏ –∏—Ö –Ω–∞–ª–∏—á–∏–∏ –¥–æ–ª–∂–Ω–∏–∫ —Å—á–∏—Ç–∞–µ—Ç—Å—è –ñ–ò–í–´–ú")
    for phrase in STOP_PHRASES[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
        print(f"   - {phrase}")
    print("   ...")
    print("=" * 80)

    if not TRANSFORMERS_AVAILABLE:
        print("‚ùå Transformers –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã")
        return

    try:
        import multiprocessing
        cpu_count = multiprocessing.cpu_count()
        print(f"\nüíª –î–æ—Å—Ç—É–ø–Ω–æ —è–¥–µ—Ä CPU: {cpu_count}")

        if torch.cuda.is_available():
            default_workers = 8
        else:
            default_workers = min(32, cpu_count * 4)

        workers_input = safe_input(f"–í–≤–µ–¥–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –Ω–∞ –¥–æ–º–µ–Ω [{default_workers}]: ").strip()
        if workers_input:
            max_workers = int(workers_input)
        else:
            max_workers = default_workers

        print(f"‚öôÔ∏è  –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {max_workers} –ø–æ—Ç–æ–∫–æ–≤ –Ω–∞ –¥–æ–º–µ–Ω")

    except:
        max_workers = 16
        print(f"‚öôÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {max_workers} –ø–æ—Ç–æ–∫–æ–≤")

    try:
        collector = FastCollector(max_workers_per_domain=max_workers)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return

    domains = collector.get_all_domains()

    if not domains:
        print("‚ùå –ù–µ—Ç –¥–æ–º–µ–Ω–æ–≤")
        return

    print(f"\nüåê –í—Å–µ–≥–æ –¥–æ–º–µ–Ω–æ–≤: {len(domains):,}")

    test_mode = safe_input("\nüî¨ –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º? (y/n): ").lower() == 'y'

    if test_mode:
        test_count = min(3, len(domains))
        domains = domains[:test_count]
        print(f"üî¨ –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º: {test_count} –¥–æ–º–µ–Ω–æ–≤")

    response = safe_input(f"\n–û–±—Ä–∞–±–æ—Ç–∞—Ç—å {len(domains)} –¥–æ–º–µ–Ω–æ–≤? (y/n): ")

    if response.lower() != 'y':
        print("–û—Ç–º–µ–Ω–µ–Ω–æ")
        return

    print("\n" + "=" * 80)
    print("‚ö° –ù–ê–ß–ò–ù–ê–ï–ú –ë–´–°–¢–†–û–ô –°–ë–û–†...")
    print("=" * 80)

    try:
        success = collector.collect_all_domains_fast(domains)

        if success:
            print("\n" + "=" * 80)
            print("‚úÖ –°–ë–û–† –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
            print("=" * 80)
            print(f"üìÅ –§–∞–π–ª: {collector.jsonl_filename}")

            with collector.stats_lock:
                print(f"üìä –ó–∞—è–≤–æ–∫: {collector.stats['total_orders']:,}")
                print(f"‚ö∞Ô∏è  –°–º–µ—Ä—Ç–µ–π: {collector.stats['death_cases']:,}")
                print(f"   üìù –ü–æ —Ç–µ–∫—Å—Ç—É: {collector.stats['death_by_text']:,}")
                print(f"   üè∑Ô∏è  –ü–æ –≥—Ä—É–ø–ø–µ: {collector.stats['death_by_group']:,}")
                print(f"   üß† –ü–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {collector.stats['death_by_neural']:,}")
                print(f"üö´ –°–ª–æ–≤-–∏—Å–∫–ª—é—á–µ–Ω–∏–π: {collector.stats['exception_words_found']:,}")
                print(f"ü™ñ –î–æ–ª–∂–Ω–∏–∫ –°–í–û: {collector.stats['svo_cases']:,}")
                print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {datetime.now() - collector.stats['start_time']}")

                if collector.stats['total_orders'] > 0:
                    total_time = (datetime.now() - collector.stats['start_time']).total_seconds()
                    speed = collector.stats['total_orders'] / total_time
                    print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {speed:.1f} –∑–∞—è–≤–æ–∫/—Å–µ–∫")

    except KeyboardInterrupt:
        print(f"\n\n‚ö†Ô∏è  –ü–†–ï–†–í–ê–ù–û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ú")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "=" * 80)
    print("üèÅ –ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print("=" * 80)


if __name__ == '__main__':
    main()
